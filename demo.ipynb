{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folding Demo\n",
    "\n",
    "This demo shows how a registered validator can:\n",
    " 1) Query the top miner uid \n",
    " 2) Demonstrate each reward/penalty mechanism/Scoring of the top miner response\n",
    " 3) Queries the API and/or links to a frontend(if applicable)\n",
    "\n",
    "In order to do this we perform the following steps:\n",
    "1. Checks wandb for currently active pdbs (preferably owned by the usersâ€™ hotkey)\n",
    "2. Uses a registered key to initialize a validator neuron\n",
    "3. Queries the miner hotkeys with the specified pdb\n",
    "4. Demonstrates how the responses are scores\n",
    "5. Plots the best configuration\n",
    "\n",
    "\n",
    "## Requirements\n",
    "In order to run this notebook you must meet the following requirements:\n",
    "1. Have a registered key on SN25 (we use opentensor as an example)\n",
    "2. Have a wandb account \n",
    "3. Have a GROMACS 2024 installed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import bittensor as bt\n",
    "\n",
    "from inspect import signature\n",
    "\n",
    "from neurons.validator import Validator\n",
    "from folding.store import Job\n",
    "from folding.validators.protein import Protein\n",
    "from folding.protocol import FoldingSynapse\n",
    "from folding.validators.reward import get_energies\n",
    "from folding.utils.ops import get_response_info\n",
    "\n",
    "WALLET_NAME = 'opentensor'\n",
    "HOTKEY_NAME = 'main'\n",
    "SUBTENSOR_NETWORK = 'finney'\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--wallet.name', type=str, default=WALLET_NAME)\n",
    "parser.add_argument('--wallet.hotkey', type=str, default=HOTKEY_NAME)\n",
    "parser.add_argument('--neuron.axon_off', type=bool, default=True)\n",
    "config = bt.config(parser=parser)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the desired wallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator = Validator(config=config)\n",
    "\n",
    "subtensor = validator.subtensor\n",
    "metagraph = validator.metagraph\n",
    "wallet = validator.wallet\n",
    "\n",
    "wallet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract information from Wandb --> Pandas.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "\n",
    "def load_run(run_path):\n",
    "\n",
    "    print('Loading run:', run_path)\n",
    "    run = api.run(run_path)\n",
    "    df = pd.DataFrame(list(run.scan_history()))\n",
    "    for col in ['updated_at', 'best_loss_at', 'created_at']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col])\n",
    "    print(f'+ Loaded {len(df)} records')\n",
    "    return df\n",
    "\n",
    "\n",
    "wandb_project = os.path.join(validator.config.wandb.entity, validator.config.wandb.project_name)\n",
    "netuid = validator.config.netuid\n",
    "max_runs = 100\n",
    "min_steps = 10\n",
    "hotkey = wallet.hotkey.ss58_address\n",
    "filters = {'state': 'running', 'config.netuid': netuid, \"tags\": {\"$in\": [hotkey]}}\n",
    "\n",
    "print(f'Searching for runs with filters: {filters}')\n",
    "\n",
    "# Grab runs on folding wandb\n",
    "for i, run in enumerate(api.runs(wandb_project, filters=filters)):\n",
    "\n",
    "\n",
    "    if i >= max_runs:\n",
    "        raise Exception(f'Exceeded max runs {max_runs}')\n",
    "\n",
    "    num_steps = run.summary.get(\"_step\")\n",
    "    print(f'run {run}, id: {run.id}, steps: {num_steps}, tags: {run.tags}')\n",
    "    \n",
    "    if num_steps is None or num_steps < min_steps:\n",
    "        continue\n",
    "\n",
    "    df = load_run('/'.join(run.path))\n",
    "    version, spec_version, hotkey, netuid_tag, *_ = run.tags\n",
    "    df['version'] = version\n",
    "    df['spec_version'] = spec_version\n",
    "    df['vali_hotkey'] = hotkey\n",
    "    df['netuid_tag'] = netuid_tag\n",
    "    df['run_id'] = run.id\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get most recent event log for my validator hotkey\n",
    "last_event = df.loc[df._step.argmax()]\n",
    "last_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in particular, we want the pdb_id of the last event and the hotkeys assigned to that job\n",
    "# NOTE: We cannot guarantee that the top miner is actually assigned to the last job\n",
    "\n",
    "pdb_id = last_event.pdb_id\n",
    "hotkeys = last_event.hotkeys\n",
    "\n",
    "rankings = metagraph.I.argsort(descending=True)\n",
    "\n",
    "uids = [metagraph.hotkeys.index(hotkey) if hotkey in metagraph.hotkeys else None for hotkey in hotkeys]\n",
    "incentives = [metagraph.I[uid].item() if uid is not None else None for uid in uids]\n",
    "rankings = [rankings[uid].item() if uid is not None else None for uid in uids]\n",
    "\n",
    "df_hotkeys = pd.DataFrame({'hotkey': hotkeys, 'uid': uids, 'incentive': incentives, 'ranking': rankings})\n",
    "\n",
    "print(f'Miners assigned to protein {pdb_id} job:')\n",
    "df_hotkeys.sort_values('incentive', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the job object from the event log\n",
    "job = Job(**{k: last_event[k] for k in signature(Job).parameters.keys() if k in last_event})\n",
    "job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the protein object from the job\n",
    "protein = Protein.from_job(job, config=None)\n",
    "protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a synapse to query the network\n",
    "synapse = FoldingSynapse(\n",
    "    pdb_id=protein.pdb_id, md_inputs=protein.md_inputs, mdrun_args=\"\"\n",
    ")\n",
    "\n",
    "axons = [metagraph.axons[uid] for uid in uids]\n",
    "\n",
    "# Make a synchronous to the network with the reconstructed protein\n",
    "responses = validator.dendrite.query(\n",
    "    axons=axons,\n",
    "    synapse=synapse,\n",
    "    timeout=10,\n",
    "    deserialize=False,  # decodes the bytestream response inside of md_outputs.\n",
    ")\n",
    "responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For now we just want to get the losses, we are not rewarding yet\n",
    "energies = get_energies(protein=protein, responses=responses, uids=uids)\n",
    "response_info = get_response_info(responses=responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
